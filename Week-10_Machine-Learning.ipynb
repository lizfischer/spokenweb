{
cells: 
[
{
cell_type: "markdown",
metadata: { },
source: 
[
"# *Unsupervised learning: Latent Dirichlet allocation (LDA) topic modeling*"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Install a Python package for LDA
",
"# http://pythonhosted.org/lda/getting_started.html
",
"
",
"!pip3 install lda"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Importing basic packages
",
"
",
"import os
",
"import numpy as np"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Downloading 'Essays' by Ralph Waldo Emerson
",
"
",
"os.chdir('/sharedfolder/')
",
"
",
"!wget http://www.gutenberg.org/cache/epub/16643/pg16643.txt"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Loading the text
",
"
",
"text_path = 'pg16643.txt'
",
"
",
"text_data = open(text_path).read()"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Dividing the document into segments, with the aim of extracting individual essays
",
"
",
"len(text_data.split('\n\n\n\n\n'))"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Viewing the beginning of each segment to determine which ones to keep
",
"
",
"counter = 0
",
"
",
"for item in text_data.split('\n\n\n\n\n'):
",
"    print('-----')
",
"    print(counter)
",
"    print(item[:80])
",
"    counter+=1"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Creating a list of essays
",
"
",
"document_list = text_data.split('\n\n\n\n\n')[9:20]
",
"
",
"print(len(document_list))"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Creating a vectorized representation of each essay in the list
",
"
",
"from sklearn.feature_extraction.text import CountVectorizer
",
"
",
"vectorizer = CountVectorizer()
",
"
",
"X = vectorizer.fit_transform(document_list) "
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Viewing a single essay's vector
",
"
",
"sample_essay_vector = X.toarray()[3]
",
"
",
"print(len(sample_essay_vector))
",
"
",
"sample_essay_vector"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Creating a vocabulary list corresponding to the vectors we created above
",
"
",
"vocabulary = vectorizer.get_feature_names()
",
"
",
"vocabulary[8950:8980]"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Viewing the 10 most frequent words in a single essay
",
"
",
"print(np.array(vocabulary)[np.argsort(sample_essay_vector)[::-1]][:10])
",
"
",
"print(np.argsort(sample_essay_vector)[::-1][:10]) # corresponding frequency values"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Initializing an LDA model: 10 topics and 1000 iterations
",
"
",
"import lda
",
"
",
"model = lda.LDA(n_topics=10, n_iter=1000, random_state=1)"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Fitting the model using our list of vectors
",
"
",
"model.fit(X)"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Viewing the top 50 words in each 'topic'
",
"
",
"topic_word = model.topic_word_
",
"
",
"n_top_words = 50
",
"
",
"for i, topic_distribution in enumerate(topic_word):
",
"    topic_words = np.array(vocabulary)[np.argsort(topic_distribution)][:-(n_top_words+1):-1]
",
"    print('Topic {}: {}'.format(i, ' '.join(topic_words)))
",
"    print()"
]
},
{
cell_type: "markdown",
metadata: { },
source: 
[
"### Repeating the process, removing stop words and punctuation first"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"from nltk.tokenize import word_tokenize
",
"
",
"word_tokenize('We are symbols, and inhabit symbols.')"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Importing NLTK stop words
",
"
",
"from nltk.corpus import stopwords
",
" 
",
"stop_words = set(stopwords.words('english'))
",
"
",
"stop_words"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Importing Python punctuation set
",
"
",
"import string
",
"
",
"string.punctuation"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Testing tokenization + stop word removal
",
"
",
"sentence = 'We are symbols, and inhabit symbols.'.lower()
",
"
",
"token_list = word_tokenize(sentence)
",
"
",
"sentence_filtered = [item for item in token_list if (item not in stop_words)&(item not in string.punctuation)]
",
"
",
"sentence_filtered"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Tokenizing and removing stop words from our list of essays
",
"
",
"documents_filtered = []
",
"
",
"for document in document_list:
",
"    token_list = word_tokenize(document.lower())
",
"    tokens_filtered = [item for item in token_list if (item not in stop_words)&(item not in string.punctuation)]
",
"    documents_filtered.append(' '.join(tokens_filtered))"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Viewing a segment of a preprocessed essay
",
"
",
"documents_filtered[3][2000:2100]"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Vectorizing preprocessed essays
",
"
",
"from sklearn.feature_extraction.text import CountVectorizer
",
"
",
"vectorizer = CountVectorizer()
",
"
",
"X = vectorizer.fit_transform(documents_filtered) "
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Creating a vocabulary list corresponding to the vectors we created above
",
"
",
"vocabulary = vectorizer.get_feature_names()
",
"
",
"vocabulary[1140:1160]"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Initializing an LDA model: 10 topics and 1000 iterations
",
"
",
"model = lda.LDA(n_topics=10, n_iter=1000, random_state=1)"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Fitting the model using our list of vectors
",
"
",
"model.fit(X)"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Viewing the top 50 words in each 'topic'
",
"
",
"topic_word = model.topic_word_
",
"
",
"n_top_words = 50
",
"
",
"for i, topic_distribution in enumerate(topic_word):
",
"    topic_words = np.array(vocabulary)[np.argsort(topic_distribution)][:-(n_top_words+1):-1]
",
"    print('Topic ' + str(i) + ':')
",
"    print(' '.join(topic_words))
",
"    print()"
]
},
{
cell_type: "markdown",
metadata: { },
source: 
[
"### ▷Assignment
",
"
",
"    Modify the code above: Apply a stemming step to each word before vectorizing the text.
",
"    See example stemming code in the following cell."
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Stemming example
",
"
",
"from nltk.stem.porter import PorterStemmer
",
"
",
"stemmer = PorterStemmer()
",
"
",
"print(stemmer.stem('nature'))
",
"
",
"print(stemmer.stem('natural'))
",
"
",
"print(stemmer.stem('naturalism'))"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: [ ]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: [ ]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: [ ]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: [ ]
},
{
cell_type: "markdown",
metadata: { },
source: 
[
"# *Supervised learning: Naive Bayes classification*"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Download sample text corpora from GitHub, then unzip.
",
"
",
"os.chdir('/sharedfolder/')
",
"
",
"!wget -N https://github.com/pcda17/pcda17.github.io/blob/master/week/8/Sample_corpora.zip?raw=true -O Sample_corpora.zip
",
"!unzip -o Sample_corpora.zip"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"os.chdir('/sharedfolder/Sample_corpora')
",
"
",
"os.listdir('./')"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Loading Melville novels
",
"
",
"os.chdir('/sharedfolder/Sample_corpora/Herman_Melville/')
",
"
",
"melville_texts = []
",
"
",
"for filename in os.listdir('./'):
",
"    text_data = open(filename).read().replace('\n', ' ')
",
"    melville_texts.append(text_data)
",
"
",
"print(len(melville_texts))"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Loading Austen novels
",
"
",
"os.chdir('/sharedfolder/Sample_corpora/Jane_Austen/')
",
"
",
"austen_texts = []
",
"
",
"for filename in os.listdir('./'):
",
"    text_data = open(filename).read().replace('\n', ' ')
",
"    austen_texts.append(text_data)
",
"
",
"print(len(austen_texts))"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Removing the last novel from each list so we can use it to test our classifier
",
"
",
"melville_train_texts = melville_texts[:-1]
",
"austen_train_texts = austen_texts[:-1]
",
"
",
"melville_test_text = melville_texts[-1]
",
"austen_test_text = austen_texts[-1]"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Creating a master list of Melville sentences
",
"
",
"from nltk.tokenize import sent_tokenize
",
"
",
"melville_combined_texts = ' '.join(melville_train_texts)
",
"
",
"melville_sentences = sent_tokenize(melville_combined_texts)
",
"
",
"print(len(melville_sentences))
",
"
",
"melville_sentences[9999]"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Extracting 2000 Melville sentences at random for use as a training set
",
"
",
"import random
",
"
",
"melville_train_sentences = random.sample(melville_sentences, 2000)"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Creating a list of Melville sentences for our test set
",
"
",
"melville_test_sentences = sent_tokenize(melville_test_text)
",
"
",
"print(len(melville_test_sentences))
",
"
",
"melville_test_sentences[997]"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Creating a master list of Austen sentences
",
"
",
"austen_combined_texts = ' '.join(austen_train_texts)
",
"
",
"austen_sentences = sent_tokenize(austen_combined_texts)
",
"
",
"print(len(austen_sentences))
",
"
",
"austen_sentences[8979]"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Extracting 2000 Austen sentences at random for use as a training set
",
"
",
"austen_train_sentences = random.sample(austen_sentences, 2000)"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Creating a list of Austen sentences for our test set
",
"
",
"austen_test_sentences = sent_tokenize(austen_test_text)
",
"
",
"print(len(austen_test_sentences))
",
"
",
"austen_test_sentences[1000]"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Combing training data
",
"combined_texts = melville_train_sentences + austen_train_sentences
",
"
",
"## Creating list of associated class values: 
",
"## 0 for Melville, 1 for Austen
",
"y = [0]*len(melville_train_sentences) + [1]*len(austen_train_sentences)"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Creating vectorized training set using our combined sentence list
",
"
",
"from sklearn.feature_extraction.text import CountVectorizer
",
"
",
"vectorizer = CountVectorizer()
",
"
",
"X = vectorizer.fit_transform(combined_texts).toarray()
",
"
",
"X.shape"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Training a multinomial naive Bayes classifier
",
"## X is a combined list of Melville and Austen sentences (2000 sentences from each)
",
"## y is a list of classes (0 or 1)
",
"
",
"from sklearn.naive_bayes import MultinomialNB
",
"
",
"classifier = MultinomialNB().fit(X, y)"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Classifying 5 sentences in our Austen test set
",
"# Recall that 0 means Melville & 1 means Austen
",
"
",
"from pprint import pprint
",
"
",
"input_sentences = austen_test_sentences[3000:3005]
",
"
",
"input_vector = vectorizer.transform(input_sentences)   ## Converting a list of string to the same
",
"                                                       ## vector format we used for our training set.
",
"pprint(austen_test_sentences[3000:3005])
",
"
",
"classifier.predict(input_vector)"
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: 
[
"## Classifying 5 sentences in our Melville test set
",
"
",
"input_sentences = melville_test_sentences[3000:3005]
",
"
",
"input_vector = vectorizer.transform(input_sentences)
",
"
",
"pprint(melville_test_sentences[3000:3005])
",
"
",
"classifier.predict(input_vector)"
]
},
{
cell_type: "markdown",
metadata: { },
source: 
[
"### ▷Assignment
",
"
",
"    Write a script that prints Austen-like sentences written 
",
"    by Melville, and Melville-like sentences written by Austen."
]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: [ ]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: [ ]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: [ ]
},
{
cell_type: "code",
execution_count: null,
metadata: { },
outputs: [ ],
source: [ ]
}
],
metadata: 
{
kernelspec: 
{
display_name: "Python 3",
language: "python",
name: "python3"
},
language_info: 
{
codemirror_mode: 
{
name: "ipython",
version: 3
},
file_extension: ".py",
mimetype: "text/x-python",
name: "python",
nbconvert_exporter: "python",
pygments_lexer: "ipython3",
version: "3.5.2"
}
},
nbformat: 4,
nbformat_minor: 2
}
+-View sourceoptions